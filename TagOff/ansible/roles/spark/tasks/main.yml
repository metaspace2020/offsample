---

- name: install ssh daemon
  apt: name=ssh state=present

- file: path=/home/{{ spark_user }}/.ssh state=directory mode=0700

- name: check if spark keys have been already generated (local)
  local_action: stat path={{ spark_key_file }}
  become: no
  register: key_file_st

- name: generate a key pair for remote ubuntu user (locally)
  become: no
  local_action: shell ssh-keygen -f {{ spark_key_file }} -t rsa -N ''
  when: key_file_st.stat.exists == False

- name: add local key to authorized_keys file
  authorized_key: user={{ spark_user }}
                  key="{{ item }}"
  with_file:
    - "{{ spark_key_file }}.pub"

- name: copy keys to the instance
  copy: src={{ item.src }} dest={{ item.dest }} owner={{ spark_user }} group={{ spark_user }} mode={{ item.mode }}
  with_items:
    - { src: "{{ spark_key_file }}", dest: "/home/{{ spark_user }}/.ssh/id_rsa", mode: "u=rw,g=,o=" }
    - { src: "{{ spark_key_file }}.pub", dest: "/home/{{ spark_user }}/.ssh/id_rsa.pub", mode: "u=rw,g=r,o=r" }

- name: Ensure Spark configuration directory exists
  file: path="{{ spark_conf_dir }}"
        state=directory
  tags: ["config"]

- name: Ensure Spark log and run directories exist
  file: path="{{ item }}"
        owner={{ spark_user }}
        group={{ spark_user }}
        mode=0755
        state=directory
  with_items:
    - "{{ spark_log_dir }}"

- name: Download Spark distribution
  get_url: url="{{ spark_mirror }}/spark-{{ spark_version }}.tar.gz"
           dest="{{ spark_src_dir }}/spark-{{ spark_version }}.tar.gz"
           timeout=1200

- name: Extract Spark distribution
  unarchive: src="{{ spark_src_dir }}/spark-{{ spark_version }}.tar.gz"
             dest="{{ spark_usr_parent_dir }}"
             remote_src=yes

- name: Setup Spark distribution symlinks
  file: src="{{ item.src }}"
        dest="{{ item.dest }}"
        state=link
  with_items:
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}", dest: "{{ spark_usr_dir }}" }
    - { src: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf", dest: "{{ spark_conf_dir }}/conf" }
  tags: ["symlinks"]

- name: Create shims for Spark binaries
  template: src=spark-shim.j2
            dest="/usr/bin/{{ item }}"
            mode=0755
  with_items:
    - spark-class
    - spark-shell
    - spark-sql
    - spark-submit
  tags: ["shims"]

- name: Configure Spark environment
  template: src=spark-env.sh.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-env.sh"
  tags: ["config"]

- name: Make "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-env.sh" executable
  file: dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-env.sh"
        mode=u=rwx,g=rx,o=rx owner={{ spark_user }} group={{ spark_user }}
  tags: ["config"]

- name: Configure Spark defaults config file
  template: src=spark-defaults.conf.j2
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/spark-defaults.conf"
            owner={{ spark_user }} group={{ spark_user }}
  tags: ["config"]

- name: Edit Spark log4j properties file
  template: src=log4j.properties
            dest="{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/log4j.properties"
            owner={{ spark_user }}
            group={{ spark_user }}
            mode=0644
